
# Copula Models {#cop-mods}

<!--
1940s Wassily Hoeffding
1951 Fréchet bounds and Fréchet classes
1959 Abe Sklar, first use of term “copula”
1958-1976 probabilistic metric spaces –> Archimedean copulas
1981 Schweizer and Wolff copulas and dependence
-->

## Notation, Definitions, and Sklar's Theorem 
<!-- Definition, Sklar's Theorem, example -->

Before defining a copula more rigorously, we review some results related to univariate probability distributions. For a random variable $Y_1$, the *distribution function* (or df) $F_1(y_1)=Pr(Y_1 \le y_1)$. Roughly speaking, this function 'adds up' the probability less than or equal to an observed value $y_1$ of the random variable. All dfs are non-decreasing, right continuous and satisfy $\lim_{y_{1} \to -\infty} F_1(y_1)=0$ and $\lim_{y_{1} \to \infty} F_1(y_1)=1$. When necessary we will use the notation $F_j(y_j;\mathbf{\gamma}_j)$ to denote the $j^{th}$ df with parameters, $\mathbf{\gamma}_j$. If $Y_1$ is a continuous random variable, $F_1(y_1)=\int_{-\infty}^{y_1}f_1(u)\,du$ where $f_1(\cdot)$ is the *density* of $Y_1$. The inverse of the df is called the *quantile function* or *inverse distribution function* and is defined as $F_1^{-1}(p)=\inf\{y_1:F_1(y_1)\ge p\}$. For a given probability $p$, the quantile function is the smallest $y_1$ such that the df evaluated at $y_1$ is at least as large as $p$. The df takes real numbers as inputs and returns probabilities while the quantile function takes probabilities and returns real numbers. 

A famous result in probability theory called the *probability integral transformation* says that if the df of a continuous random variable is applied to the random variable itself, the resulting transformed random variable has a continuous standard uniform distribution. So $F_1(Y_1)=U_1$ where $U_1 \sim Unif(0,1)$ (see [Technical Supplement](#tech-supp)). This result also implies that given a standard uniform random variable and the quantile function, we can generate the df of the original random variable since $F_{1}^{-1}(U_1)=F_{1}^{-1}(F_1(Y_1))=Y_1$.

When dealing with multivariate outcomes we are interested in several random variables, so we define $Y_j$ to be the random variable representing the $j^{th}$ outcome where $j=1,\ldots,d$ and the dimension, $d$, is the number of outcomes of interest. In the bivariate case $d=2$. Let $F_j$ and $F_j^{-1}$ be the df and quantile function associated with $Y_j$, respectively, and $y_j$ be an observed value of this random variable. Finally, define the *multivariate distribution function* for the $Y_j$ to be $H(\mathbf{y})=Pr(Y_1 \le y_1, \ldots, Y_d \le y_d)$ where $\mathbf{y} = (y_1,\ldots,y_d)$ is a vector of observed values for $Y_1$ through $Y_d$. This function gives the joint probability of being less than or equal to all the observed $y_j$ simulataneously. <!--[insert example related to trials??]-->

Given a multivariate df $H$, the individual univariate df $F_j$ can be recovered by evaluating $H$ as the limits for the other random variables go to infinity. In the two dimensional case, $F_1(y_1)=\lim_{y_2 \to \infty}H(y_1,y_2)$ and $F_2(y_2)=\lim_{y_1 \to \infty}H(y_1,y_2)$. Because of this, the $F_j$ are often called the *marginal* dfs. However, given univariate dfs $F_j$ it is not immediately clear how they relate to $H$. A foundational 1959 paper by mathematician Abe Sklar [@sklar_fonctions_1959] provides the answer in the form of an equation that relates the univariate dfs, the copula $C$, and $H$. It also guarantees that if the univariate dfs are continuous $H$ is *unique*. The same theorem implies the converse; given $d$ univariate dfs and $H$, the equation can be used to construct a unique $C$.

### Sklar's theorem

For a $d$-variate distribution $H$ with $j^{th}$ univariate margin $F_j$, the copula associated with $H$ is a distribution function $C: [0,1]^d \to [0,1]$ with $Unif(0,1)$ margins that satisfies:
\begin{gather}
H(\mathbf{y})=C(F_1(y_1),\ldots,F_d(y_d)), \, \mathbf{y} \in \mathbb{R}^d (\#eq:eq2-1)
\end{gather}
(a) If $H$ is a continuous $d$-variate distribution with univariate margins $F_1,\ldots, F_d$, and corresponding quantile functions $F_1^{-1},\ldots, F_d^{-1}$, then
\begin{gather}
C(\mathbf{u})=H(F_1^{-1}(u_1),\ldots, F_d^{-1}(u_d)),\, \mathbf{u} \in [0,1]^d (\#eq:eq2-2)
\end{gather}
is the unique choice.  

(b) If $H$ is a $d$-variate distribution with discrete (or a mix of discrete and continuous) univariate margins, then the copula is unique only on the set
\begin{gather}
Range(F_1) \times \cdots \times Range(F_d) \text{ where } Range(F_j)=\{F_j(x): x \in \mathbb{R}\}(\#eq:eq2-3)
\end{gather}

Equations \@ref(eq:eq2-1) and \@ref(eq:eq2-2) elucidate the link between the marginal dfs $F_1$ through $F_d$ and the joint df $H$, linked by the copula $C$. In \@ref(eq:eq2-1), the probability integral transform ensures that $F_j(y_j) = u_j$ are observed values from a standard uniform distribution if $Y_j$ are continuous random variables. If one or more of the marginal dfs are discrete, the copula is only unique on the set defined in equation \@ref(eq:eq2-3). Informally, the range (also called the image) of a df is the subset of all the numbers to which the function maps. If the dfs are continuous, the range includes all real numbers between 0 and 1, but the output of a discrete df includes only a finite subset of numbers in the interval $[0,1]$. So a copula can be constructed with discrete margins, but it is only uniquely defined at certain values. Genest and Neslehova [@genest_primer_2007] describe many potential problems which can arise when using copulas with discrete margins. For example, the non-uniqueness of $C$ can lead to identifiability issues in estimation and the dependence between variables is a function of both the copula and the margins, undercutting one of the key benefits of copula models. Denuit and Lambert [@denuit_constraints_2005] describe restrictions to the range of dependence measures, such as Kendall's $\tau$, when the margins are bivariate discrete random variables. Further, the usual probability integral transform does not hold, so more exotic transformations are needed to obtain uniform distributions [@niewiadomska-bugaj_grade_2005; @ruschendorf_distributional_2009].  In many applications, including the two explored in the next section, some of these issues are avoided by positing a latent continuous variable underlying the discrete outcome.

Just as all univariate dfs must satisfy certain properties, each copula must satisfy the following:  

1. *Grounded*: $C(u_1,\ldots, u_d)=0$ if $u_j=0$ for at least one $j$    
2. *Standard Uniform Margins*: For any $j$, $C(1,\ldots,1,u_j,1,\ldots,1)=u_j$ for all $u_j \in [0,1]$   
3. *Non-negative C-volume*: For $d=2$, let $[a_1,a_2] \times [b_1,b_2]$ be a rectangle in the unit square $[0,1]^2$ with lower end point $[a_1,a_2]$ and upper end point $[b_1,b_2]$. The *C-volume*, $\Delta_{(\mathbf{a},\mathbf{b}]}C=C(b_1,b_2)-C(b_1,a_2)-C(a_1,b_2)+C(a_1,a_2)$. For a copula, the C-volume must be nonnegative for all possible rectangles $[a_1,a_2] \times [b_1,b_2]$. (see [Technical Supplement](#tech-supp) for $d>2$)  

<!--
[See https://cran.r-project.org/web/packages/copula/vignettes/empiricial_copulas.html Example 3 and Genest & Neslehova paper for examples/explanation of non-uniqueness with discrete copula] 

Genest and Neslehova [@genest_primer_2007] describe many limitations and potential problems regarding copulas with discrete models which will mention during the review of copula concepts in the next section. 
-->

A given set of dfs $F_1,\ldots,F_d$ can correspond to many different multivariate dfs. This was illustrated in the previous section where identical normal and gamma margins produced two different bivariate dfs when combined with different copulas. The class of all the multivariate dfs with fixed margins $F_1,\ldots,F_d$ is called the *Fréchet class*. Similarly, the class of all dfs that can be derived from a given copula, $C$, is called a *meta-C* model. Consider the bivariate normal (or Gaussian) copula with correlation $\rho$:
\begin{gather}
C_{\rho}^{Norm}(u_1,u_2)=\Phi_2(\Phi^{-1}(u_1),\Phi^{-1}(u_2)|\rho)
=\!\! \int_{-\infty}^{\Phi^{-1}(u_1)}\!\!\int_{-\infty}^{\Phi^{-1}(u_2)} \!\! \frac{1}{2 \pi \sqrt{1-\rho^2}} \exp \left[ \frac{-(x^2-2\rho xy + y^2)}{2(1-\rho^2)}\right] dx dy (\#eq:eq2-4)
\end{gather}
where $\Phi_2(\cdot|\rho)$ is the bivariate standard normal df with correlation coefficient $\rho$, $\Phi^{-1}$ is the quantile function of the univariate standard normal df and $u_j \in [0,1]$ for $j=1,2$. If the margins are both standard normal then $u_1=\Phi(z_1)$ and $u_2=\Phi(z_2)$ and expression \@ref(eq:eq2-4) becomes $C_{\rho}^{Norm}(\Phi(z_1),\Phi(z_2))=\Phi_2(\Phi^{-1}(\Phi(z_1)),\Phi^{-1}(\Phi(z_2))|\rho)=\Phi_2(z_1,z_2|\rho)$ which is the usual bivariate normal df. In other words, a multivariate normal distribution can be represented as a combination of normal margins with a normal copula. Using margins which are not standard normal will produce a distribution that is *not* multivariate normal. However the dependence between the two margins -- contained in the copula -- is identical in both scenarios. This is demonstrated in Figure \@ref(fig:norm-cop-ex) where the same normal copula is used to construct two multivariate dfs (both members of meta-$C_{R}^{Norm}$ model) by specifying different marginal dfs.

```{r norm-cop-ex, cache=TRUE, fig.cap='n=2000 random draws from the bivariate distribution constructed with a normal copula and (1) standard normal margins or (2) Student-$t_3$ and $exp(1)$ margins. True $\\rho=0.6322$ and $\\tau=0.4505$', fig.show = 'hold', fig.align='center', out.width='45%'}
## normal copula example
set.seed(2734)

# define normal copula
nc <- normalCopula(0.65)

# multivariate distributions combining margins and copula
mvd_nc1 <- mvdc(copula = nc, margins = c("norm", "norm"),
              paramMargins = list(list(mean = 0, sd = 1), 
                                  list(mean = 0, sd = 1)))

mvd_nc2 <- mvdc(copula = nc, margins = c("t", "exp"),
              paramMargins = list(list(df=3), 
                                  list(rate = 1)))

#calculate true rho and tau
if (0){ rho(nc); tau(nc) }

# plot joint densities
#contour(mvd_nc1, dMvdc, xlim=c(-4,4), ylim=c(-4,4), 
#      xlab="Normal", ylab="Normal")

#magnify plots
cx <- 1.5

mvd_nc1_samps <- rMvdc(2000, mvd_nc1)
mvd_nc1_rho <- round(cor(mvd_nc1_samps, method="spearman"),3)
mvd_nc1_tau <-round(cor(mvd_nc1_samps, method="kendall"),3)

leg1<-c(as.expression( bquote(hat(rho)~"="~.(mvd_nc1_rho[1,2])) ), 
        as.expression( bquote(hat(tau)~"="~.(mvd_nc1_tau[1,2]))))

plot(mvd_nc1_samps, pch=16, col=rgb(0,0,0,0.2), xlab="(1)", ylab="", 
     cex=cx, cex.axis=cx, cex.lab=cx)
legend("topleft",legend=leg1, cex=cx)

# plot joint density
#contour(mvd_nc2, dMvdc, xlim=c(-4,4), ylim=c(-2,6), 
#      xlab="Student t", ylab="Exponential")

mvd_nc2_samps <- rMvdc(2000, mvd_nc2)
mvd_nc2_rho <- round(cor(mvd_nc2_samps, method="spearman"),3)
mvd_nc2_tau <- round(cor(mvd_nc2_samps, method="kendall"),3)

leg2<-c(as.expression( bquote(hat(rho)~"="~.(mvd_nc2_rho[1,2])) ), 
        as.expression( bquote(hat(tau)~"="~.(mvd_nc2_tau[1,2]))))

plot(mvd_nc2_samps, pch=16, col=rgb(0,0,0,0.1), xlab="(2)", ylab="", 
     cex=cx, cex.axis=cx, cex.lab=cx)
legend("topleft",legend=leg2, cex=cx)
```

## Copula Concepts
<!--
Important copula concepts and background

see Schweizer [@schweizer_nonparametric_1981] for attribution of certain copula results
-->

Because of its role in tying together univariate dfs, the copula provides a good framework to study properties related to association between variables. Many dependency properties can be expressed in terms of copulas and there is a diverse array of copulas available to represent different dependence structures.

### Independence and Bounds

One of the most important relationships among outcomes is independence or the absence of association, which is defined by the *independence copula* (also called the product copula) $\Pi(\mathbf{u})=\prod_{j=1}^{d} u_j, \,\mathbf{u} \in [0,1]^d$. Given continuous random variables $Y_1$ and $Y_2$ with unique copula $C$, $Y_1$ and $Y_2$ are independent if and only if $C=\Pi$. In this case $H(y_1,y_2)=C(F_1(y_1),F_2(y_2))=\Pi(F_1(y_1),F_2(y_2))=F_1(y_1)F_2(y_2)$.

In addition, limits on dependence can be expressed in terms of copulas by the *Fréchet-Hoeffding bounds*. For $\mathbf{u} \in [0,1]^d$, the upper (or comonotonic) bound is $M(\mathbf{u})=\min\{u_1,\ldots,u_d\}$. $M$ is itself a copula and defines *perfect positive dependence* between variables. In this situation, values of all variables increase together almost surely. A simple form of this dependence is a linear relationship between outcomes $Y_1=a +b Y_2$ with $b>0$, but many other relationships are also comonotonic. In the case of $d=2$, the lower (or countermonotonic) bound is $W(\mathbf{u})=\max\{\sum_{j=1}^{d} u_j -d + 1, 0\}$ and the copula $W$ defines *perfect negative dependence* between variables; an increase in the first variable is associated with a decrease in the other variable almost surely. For $d\ge 3$, $W$ is not a copula and, in fact, there are no countermonotonic copulas since perfect negative dependence between the first two variables would rule out simultaneous perfect negative dependence with a third variable. Despite this $W$ is still a bound on the values a copula can attain when $d\ge 3$. Putting these two bounds together we have $W(\mathbf{u}) \le C(\mathbf{u}) \le M(\mathbf{u})$ for any copula $C$.

```{r cop-bounds-html, eval=knitr::is_html_output(), fig.cap='Lower and Upper Copula bounds'}
# code modified from http://copula.r-forge.r-project.org/book/02_copulas.html

u <- seq(0, 1, length.out = 40) # subdivision points in each dimension
u12 <- expand.grid("u[1]" = u, "u[2]" = u) # build a grid
W <- pmax(u12[,1] + u12[,2] - 1, 0) # values of W on grid
M <- pmin(u12[,1], u12[,2]) # values of M on grid
val.W <- cbind(u12, "W(u[1],u[2])" = W) # append grid
val.M <- cbind(u12, "M(u[1],u[2])" = M) # append grid

cont_opt2 <- list(x = list(show=FALSE, usecolormap=FALSE, highlight=FALSE),
                 y = list(show=FALSE, usecolormap=FALSE, highlight=FALSE),
                 z = list(show=FALSE, usecolormap=FALSE, highlight=FALSE))

scene_opt2 <- list( xaxis = list(title = "u1"), yaxis = list(title = "u2"),
                 zaxis = list(title = ""),
                 camera = list(eye = list(x = -1.75, y = -1.75, z = 1.25)))

# updatemenus component
updatemenus <- list(
  list(
    active = -1,
    type= 'buttons',
    buttons = list(
      list(
        label = "Upper Bound, M",
        method = "update",
        args = list(list(visible = c(FALSE, TRUE)), list(title = "Upper Bound, M"))),
      list(
        label = "Lower Bound, W",
        method = "update",
        args = list(list(visible = c(TRUE, FALSE)), list(title = "Lower Bound, W"))),
      list(
        label = "Both Bounds",
        method = "update",
        args = list(list(visible = c(TRUE, TRUE)), list(title = "Both Bounds"))))
  )
)

plot_ly(x = u, y = u, hoverinfo='none') %>% 
  add_surface(z = t(matrix(val.W$`W(u[1],u[2])`,nrow=40, byrow=FALSE)), opacity = 0.75,
              showscale=FALSE, contours = cont_opt2) %>%
  add_surface(z = t(matrix(val.M$`M(u[1],u[2])`,nrow=40, byrow=FALSE)), opacity = 0.65,
              showscale=FALSE, contours = cont_opt2) %>%
  layout(scene = scene_opt2, updatemenus = updatemenus)
```

### Invariance

Another important feature of copulas is invariance to monotone increasing transformations. For $(Y_1,\ldots,Y_d) \sim H$ with continuous margins $F_1,\ldots,F_d$ and copula $C$ if $T_j$ is a strictly increasing transformation on $Range(X_j)$ then $(T_1(Y_1),\ldots,T_d(Y_d))$ also has copula $C$. This means that for strictly increasing transformations, as stated by Schweizer and Wolff [@schweizer_nonparametric_1981], "the copula is invariant while the margins may be changed at will, it follows that it is precisely the copula which captures those properties of the joint distribution which are invariant under a.s. strictly increasing transformations." For instance, the copula between $Y_1$ and $Y_2$ is the same as the copula between $\log(Y_1)$ and $\log(Y_2)$.
 
<!--
```{r, eval=FALSE, echo=FALSE}
# copulas are invariant to strictly increasing functions
# from Hofert sec. 2.4
rho <- 0.6
P <- matrix(c(1, rho, rho, 1), ncol = 2) # the correlation matrix
C <- function(u) pCopula(u, copula = normalCopula(rho)) # normal copula

Htilde <- function(x)
    apply(cbind(x[,1], -log((1-x[,2])/x[,2])), 1, function(x.)
          pmvnorm(upper = x., corr = P))
qF1tilde <- function(u) qnorm(u)

if (0){
Htilde <- function(x)
    apply(cbind(log(x[,1]), -log((1-x[,2])/x[,2])), 1, function(x.)
          pmvnorm(upper = x., corr = P))
qF1tilde <- function(u) exp(qnorm(u))
}
qF2tilde <- function(u) 1/(1+exp(-qnorm(u)))
Ctilde <- function(u) Htilde(cbind(qF1tilde(u[,1]), qF2tilde(u[,2])))
set.seed(31)
u <- matrix(runif(5 * 2), ncol = 2) # 5 random evaluation points
stopifnot(all.equal(Ctilde(u), C(u)))

```
-->

### Dependence Measures  

Many well known measures of association such as Spearman's $\rho$, Kendall's $\tau$, and the Gini rank correlation coefficient can be expressed in terms of copulas. A conspicuous absence from this list is the most common association measure, the Pearson correlation coefficient, $r$. While there is a relationship between the copula, its margins and $r$ in certain cases, Pearson correlation cannot be expressed in terms of the copula alone. However, the correlation is not a good comprehensive measure of association since it only gives the strength of linear dependence, does not exist for every random vector $(Y_1, Y_2)$, and is not invariant to strictly increasing transformations. 

All of these measures summarize the multivariate dependence into a single number meant to convey how strongly 'large' values of one variable are associated with 'large' values of the other variable. To be more precise, Spearman's $\rho$ and Kendall's $\tau$ are both measures of concordance. Two i.i.d. pairs $(Y_1,Y_2)$ and $(Y_1',Y_2')$ with joint distribution $H$, copula $C$, and common marginal dfs $F_1$ and $F_2$, are concordant if $(Y_1-Y_1')(Y_2-Y_2')>0$ and disconcordant if $(Y_1-Y_1')(Y_2-Y_2')<0$.
 
The population measure of Kendall's $\tau$ is the probability of concordance minus the probability of discordance, so $\tau=Pr[(Y_1-Y_1')(Y_2-Y_2')>0] - Pr[(Y_1-Y_1')(Y_2-Y_2')<0]$. We can express the same measure in terms of copulas as $\tau = 4 \iint_{[0,1]^2} C(u_1,u_2)\, dC(u_1,u_2) - 1$.

The population measure of Spearman's $\rho$ can be defined by considering a third pair $(Y_1'',Y_2'')$. Note that the pairs $(Y_1,Y_2)$ and $(Y_1',Y_2'')$ have the same margins, but the first pair has joint distribution $H$ while the second is independent (since $(Y_1',Y_2')$ and $(Y_1'',Y_2'')$ are i.i.d.). Then $\rho=3(Pr[(Y_1-Y_1')(Y_2-Y_2'')>0] - Pr[(Y_1-Y_1')(Y_2-Y_2'')<0])$. In terms of the copula, $\rho=12 \iint_{[0,1]^2}[C(u_1,u_2) -u_1u_2]\,du_1du_2 = 12\iint_{[0,1]^2}C(u_1,u_2)\,du_1du_2 - 3$. The first equality shows that Spearman's $\rho$ is the 'average' distance between the distribution of $Y_1$ and $Y_2$ (represented by $C$) and independence (represented by $\Pi$).

<!--
Scarsini measures of concordance, desirable traits (see Joe p 54)

Other dependence measures:
Blomqvist's $\beta$ (see Joe p 57)
Hoeffding's Dependence coefficient $D$ (see Ozwar pdf p8 and https://en.wikipedia.org/wiki/Hoeffding%27s_independence_test)
Gini rank correlation coefficient (see Ozwar pdf p8 and https://doi-org.proxy.library.vanderbilt.edu/10.1080/10485259808832744)

For discrete margins, measures of association may depend on the margins in this scenario, which is counter to the goal of separating marginal and dependence model specification
[@denuit_constraints_2005] ==> can no longer directly interpret copula parameter in terms of dependence
-->

Another dependence measure often studied in the context of copulas is *tail dependence* which quantifies the strength of dependence in the joint tails of the multivariate distribution. Multivariate dfs with 'heavier' tails will place higher probability on extreme events occuring simultaneously. Figure \@ref(fig:tail-dep-ex) (reproduced using code from [@hofert_elements_2018]) shows four samples with identical margins and Kendall's $\tau$, but different copulas. The differences in the proportion of samples in the bivariate tails provides some insight into the concept of tail dependence. The normal copula has no tail dependence (unless the variables are comonotonic), while the Student-$t$ has symmetric upper and lower tail dependence. The Gumbel copula allows upper tail dependence and the Clayton copula allows lower tail dependence. 

Lower tail dependence is defined as $\lambda_l = \lambda_l(Y_1,Y_2)= \lim_{q\downarrow 0} Pr(Y_2 \le F_{2}^{-1}(q)|Y_1 \le F_{1}^{-1}(q))$,
i.e., the limit as $q$ goes to 0 of the conditional probability of $Y_2$ being smaller than the $q^{th}$ quantile given that $Y_1$ is smaller than the $q^{th}$ quantile. In terms of the copula between $(Y_1,Y_2)$, $\lambda_l(C)=\lim_{q\downarrow 0}\frac{C(q,q)}{q}$. If $\lambda_l \in (0,1]$, then $Y_1$ and $Y_2$ are *lower tail dependent*. Similarly, upper tail dependence is defined as $\lambda_u=\lim_{q\uparrow 1} Pr(Y_2 > F_{2}^{-1}(q)|Y_1 > F_{1}^{-1}(q))=\lim_{q\uparrow 1} \frac{1-2q + C(q,q)}{1-q}$. If $\lambda_u \in (0,1]$, then $Y_1$ and $Y_2$ are *upper tail dependent*. 

```{r tail-dep-ex, cache=TRUE, fig.cap='Four samples of n=10000 random draws from bivariate distributions constructed with standard normal margins and Kendall\'s $\\tau=0.7$ with (clockwise from top left) normal copula, Student-$t_3$ copula, Gumbel copula and Clayton copula. Dashed lines indicate the 0.005 and 0.995 quantiles of the standard normal margins', fig.show = 'hold', fig.align='center', out.width='45%'}

### Four distributions with N(0,1) margins and a Kendall's tau of 0.7

## Kendall's tau and corresponding copula parameters
tau <- 0.7
th.n <- iTau(normalCopula(),  tau = tau)
th.t <- iTau(tCopula(df = 3), tau = tau)
th.c <- iTau(claytonCopula(), tau = tau)
th.g <- iTau(gumbelCopula(),  tau = tau)
## Samples from the corresponding 'mvdc' objects
set.seed(271)
n <- 10000
N01m <- list(list(mean = 0, sd = 1), list(mean = 0, sd = 1)) # margins
Y.n <- rMvdc(n, mvdc = mvdc(normalCopula(th.n),    c("norm", "norm"), N01m))
Y.t <- rMvdc(n, mvdc = mvdc(tCopula(th.t, df = 3), c("norm", "norm"), N01m))
Y.c <- rMvdc(n, mvdc = mvdc(claytonCopula(th.c),   c("norm", "norm"), N01m))
Y.g <- rMvdc(n, mvdc = mvdc(gumbelCopula(th.g),    c("norm", "norm"), N01m))

##' @title Function for producing one scatter plot
##' @param Y data
##' @param qu (lower and upper) quantiles to consider
##' @param lim (x- and y-axis) limits
##' @param ... additional arguments passed to the underlying plot functions
##' @return invisible()
plotCorners <- function(Y, qu, lim, smooth = FALSE, ...)
{
    plot(Y, xlim = lim, ylim = lim, xlab = quote(Y[1]), ylab = quote(Y[2]),
         col = adjustcolor("black", 0.5), ...) # or pch = 16
    abline(h = qu, v = qu, lty = 2, col = adjustcolor("black", 0.6))
    ll <- sum(apply(Y <= qu[1], 1, all)) * 100 / n
    ur <- sum(apply(Y >= qu[2], 1, all)) * 100 / n
    mtext(sprintf("Lower left: %.2f%%, upper right: %.2f%%", ll, ur),
          cex = 0.9, side = 1, line = -1.5)
    invisible()
}
## Plots
a. <- 0.005
q <- qnorm(c(a., 1 - a.)) # a- and (1-a)-quantiles of N(0,1)
lim <- range(q, Y.n, Y.t, Y.c, Y.g)
lim <- c(floor(lim[1]), ceiling(lim[2]))
plotCorners(Y.n, qu = q, lim = lim, cex = 0.4)
plotCorners(Y.t, qu = q, lim = lim, cex = 0.4)
plotCorners(Y.c, qu = q, lim = lim, cex = 0.4)
plotCorners(Y.g, qu = q, lim = lim, cex = 0.4)
```

In practical terms, understanding the dependence properties of the joint distribution under investigation will help guide the choice of an appropriate copula or copulas to model these properties.

### Copula Familes 

Two of the most popular classes of copula are the elliptical copulas, which include multivariate normal and multivariate Student-$t$ copulas, and the Archimedean copulas. Elliptical distributions have density functions whose contours are concentric ellipses with constant eccentricity. Elliptical copulas are derived by replacing $H$ and $F_1^{-1},\ldots,F_d^{-1}$ in equation \@ref(eq:eq2-2) with the corresponding multivariate df and quantile functions from the elliptical distribution as shown in \@ref(eq:eq2-4) for the normal distribution. Archimedean copulas can be written in the form $C(u_1,\ldots,u_d)=\psi^{[-1]}(\psi(u_1) + \cdots + \psi(u_d))$ where $\psi$ is called the *generator* function. If the generator function satisfies certain requirements, the formula in the previous sentence will produce a valid copula. 

Table `r if (knitr::is_html_output()) '\\@ref(tab:tab1-html)' else '\\@ref(tab:tab1)'` shows common copula families along with desirable properties. A closed-form df is helpful for evaluating discrete margins since probabilities are differences in dfs for discrete variables. The desirability of the other properties depends on the dependence structure of the data to be modeled. No family possesses all the desirable properties, but there are a variety of methods for constructing copulas with a combination of desirable characteristics. One method is through mixture copulas, which are weighted combinations of copulas. For example $C^{mix}(\mathbf{u}|R,\theta_{G},\theta_{C},w_1,w_2)=w_1C_{R}^{Norm}(\mathbf{u}|R) + w_2C^{Gumbel}(\mathbf{u}|\theta_{G}) + w_3C^{Clayton}(\mathbf{u}|\theta_{C})$ where $w_1,w_2$ and $w_3=1-w_1-w_2$ are mixture weights. $C^{mix}$ can accomodate symmetry and both lower and upper tail dependence, but at the price of additional complexity. Joe [@joe_multivariate_1996] presents another technique called 'mixture of max-id' for creating closed form copulas from mixtures that exhibit flexibility in dependence, tail dependence and tail asymmetry. The copulas presented here are only a small subset of those which have been described, e.g. Nadarajah et al. [@nadarajah_compendium_2018] provides details on over 60 copulas.
 
 <!--
-Comprehensive: contains W, M, and independence copulas , e.g. Plackett (Nelsen 3.3.3), Frechet family (Nelsen ex 2.4) 
-Combinations: mixtures of copulas are also copulas 
-Sampling/simulation
-Generating copulas
-->

```{r tab1-0, echo=FALSE}
#for more info about tables
#https://haozhu233.github.io/kableExtra/
#https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf
#https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
#https://haozhu233.github.io/kableExtra/bookdown/cross-format-tables-in-bookdown.html

tab1_entries<-list(c('Normal/Gaussian','Student-t','Gumbel','Clayton','Frank'),
               c('Elliptical','Elliptical','Archimedean','Archimedean','Archimedean'),
               c('Yes','No','Yes','Yes','Yes'),
               c('Yes','Yes','No','No','Yes'),
               c('No','Yes','Yes, upper tail', 'Yes, lower tail','No'),
               c('No','No','Yes','Yes','Yes'),
               c('Yes','Yes','Yes','Yes','Yes'))

tab1<-matrix(unlist(tab1_entries), byrow=FALSE, ncol=length(tab1_entries))
```

```{r tab1, eval=knitr::is_latex_output(), echo=FALSE, results='asis'}
kable(tab1, "latex", booktabs = TRUE, escape=FALSE, 
      caption = 'Common Copulas and their Properties',
             col.names = linebreak(c("Name","Class","Includes $M$ \nand $\\Pi$",
             "Symmetric","Tail Dependence","Closed-form \ndf", "Closed-form \ndensity"),
             align = "c"))
```

```{r tab1-html, eval=knitr::is_html_output(), echo=FALSE}
kable(tab1, "html", booktabs = TRUE, escape=FALSE, 
      caption = 'Common Copulas and their Properties',
             col.names = c("Name","Class","Includes $M$ \nand $\\Pi$",
             "Symmetric","Tail Dependence","Closed-form \nCDF", "Closed-form \ndensity"),
             align = "c")
```

## Inference
<!--
Given data and a joint model specified by margins and copula, how to estimate marginal and copula parameters and perform inference
-->

Equation \@ref(eq:eq2-1) shows how to construct a multivariate df from univariate dfs and a copula, but in practice neither the margins nor the copula is known. Rather we collect $n$ multivariate observations $(y_{i1},\ldots,y_{id}),\; i=1,\ldots,n$ from which the unknown parameters are inferred. In clinical trials, the margins are often of primary interest and the general form of each margin is assumed to be known. The inference task is to find the best multivariate df among the *Fréchet class* with the given margins. The resulting estimated parameters and multivariate df can be used to determine the effect of intervention on each individual outcome and the dependence between outcomes. We briefly describe how fully parametric inference is performed for copulas under frequentist and Bayesian paradigms although semiparametric and nonparametric methods have also been developed [see @hofert_elements_2018, ch. 5; @joe_dependence_2015, ch. 4; @hoff_extending_2007; @wu_bayesian_2014; @ning_nonparametric_2017]. For both paradigms the log-likelihood function is central. In the simplest case of all continuous marginal dfs $F_1,\ldots,F_d$,
differentiation with respect to all $d$ dimensions can be used to obtain the copula density $c(u_1,\ldots,u_d)=\frac{\partial^d C(u_1,\ldots,u_d)}{\partial u_1 \cdots \partial u_d}$. Then the multivariate density is $f(\mathbf{y})=c(F_1(y_1),\ldots,F_d(y_d)) \times \prod_{j=1}^{d} f_j(y_j)$. Assuming the multivariate observations $(y_{i1},\ldots,y_{id})$ are i.i.d., the log-likelihood is:
\begin{gather}
\ell(\mathbf{\gamma}_1,\ldots,\mathbf{\gamma}_d,\mathbf{\theta}) = \sum_{i=1}^{n} \{ \log c_{\theta}(F_1(y_{i1};\mathbf{\gamma}_1),\ldots,F_d(y_{id};\mathbf{\gamma}_d)) + \sum_{j=1}^{d} \log f_j(y_{ij};\mathbf{\gamma}_j) \}(\#eq:eq2-ll)
\end{gather}

Likelihood functions for discrete or mixed margins are available in the [Technical Supplement](#tech-supp).

### Frequentist

Maximizing equation \@ref(eq:eq2-ll) yields the maximum likelihood estimates, $(\mathbf{\hat{\gamma}}_1,\ldots,\mathbf{\hat{\gamma}}_d,\mathbf{\hat{\theta}})$. The maximization can be performed jointly for the entire likelihood or in a two-stage estimation procedure called *inference functions for margins* [@joe_estimation_1996]. Inference functions for margins first obtains estimates for all marginal parameters separately and then uses these estimates as fixed values for the margins to estimate the copula parameters. This approach is less statistically efficient than the full maximum likelihood estimator and does not properly account for the variability in the marginal estimates when estimating the copula parameters, but is more computationally efficient if $d$ is large since it avoids maximization over a high dimensional parameter space.

### Bayesian

Bayesian inference provides a principled method of combining prior information concerning parameters with observed data based on Bayes' Theorem: 
\begin{gather}
p(\theta|y)=\frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta)\,d\theta} (\#eq:bayes1)
\end{gather}
For copula modeling, $p(y|\theta)$ in equation \@ref(eq:bayes1) is replaced by the likelihood from \@ref(eq:eq2-ll) and $p(\theta)$ represents prior information (or absence of information) about the marginal and copula parameters. The combination of these two pieces yields the posterior probability $p(\theta|y)$ which represents an updated distribution for the parameters conditional on the observed data.

Performing inference for copula models in a Bayesian framework has several advantages. Principle among these is the ability to answer inferential questions using combinations of interpretable posterior probabilities rather than the hypothesis tests and p-values which are dominant in the frequentist paradigm. Bayesian models correctly propagate uncertainty by accounting for the variance in the marginal models when estimating copula parameters [@craiu_mixed_2012]. Since the posterior can be iteratively updated as new data are collected, they are also attractive for adaptive or sequential designs [@yuan_bayesian_2011]. Smith [@smith_estimation_2012; @damien_bayesian_2013] describes several circumstances where a Bayesian copula model may be preferred. These include when the likelihood is difficult to maximize but the posterior can be approximated well using a Monte Carlo approach, when the focus of inference is a measure of dependence, quantile, or other quantity for which confidence intervals or p-values are difficult to obtain, when using hierarchical models of copula dependence structure or margins, and to perform shrinkage/penalization in models with a large number of parameters. Finally, by developing appropriate priors Bayesian models can include information from different sources including previous related studies, pre-clinical models, elicited clinical expertise, or observational data sources, although these extensions have not been well explored in the context of copula models.

<!--
Other benefits: Bayesian data augmentation []
-->

## Copula Regression
<!-- Extension to regression -->

In order to use copulas in a clinical trial setting, the basic theory must be extended to include covariates for the groups being compared (dose levels, treatment vs. control, etc.) and possibly adjust for other factors.

### Regression models for marginal outcomes

The margins can be extended to depend on covariates by replacing each marginal df with a conditional df tied to a regression model. $F_j(Y_j)$ is replaced by $F_j(Y_j|X_j)$ where $X_j$ is a vector of $p_j$ known covariates for the $j^{th}$ marginal regression model and $x_j$ is an observed value of the $j^{th}$ vector of covariates. In a clinical trial setting the covariate of interest is often treatment or dose level and this covariate would be common to all margins, however the $j$ marginal models are not required to have a common set of covariates or to use the same functional form for covariates. For each $j=1,\ldots,d$ there is a *marginal calibration function* $\varphi_j$ which relates the observed $\mathbf{x}_j$ to the parameters of $F_j$. One possible marginal calibration function for location parameter of $F_j$ is $\gamma_{j,\mathbf{x}_j}=\varphi_j(\mathbf{x}_j)=E[Y_j|X_j=\mathbf{x}_j]=\beta_{j0}+\sum_{k=1}^{p_j} \beta_{jk}x_{jk}$. This is equivalent to a OLS regression model for the margin. The conditional df based on this calibration function is written $F_{j}(y_j;\varphi_j(\mathbf{x}_j))$.

<!--
survey of copula-based regression models [@kolev_copula-based_2009]
-->

### Regression models for copula parameters

Covariates can also be used to model the copula dependence parameter, using a *copula calibration function*, $\theta_{\mathbf{x}}=\zeta(\mathbf{x})$. Including both the marginal and copula calibration functions the likelihood from equation \@ref(eq:eq2-ll) becomes:
\begin{gather}
\ell(\mathbf{\beta}_1,\ldots,\mathbf{\beta}_d,\mathbf{\theta}) = 
\sum_{i=1}^{n} \{ \log c_{\theta_{\mathbf{x}}}(F_1(y_{i1}; \varphi_1(\mathbf{x}_{i1})), \ldots , F_d(y_{id};\varphi_d(\mathbf{x}_{id}))) + 
\sum_{j=1}^{d} \log f_j(y_{ij};\varphi_j(\mathbf{x}_{ij})) \}(\#eq:eq2-ll2)
\end{gather}

Models where the dependence structure depends on covariates are called *conditional copulas* and are an active area of research [@craiu_mixed_2012; @acar_statistical_2013; @levi_gaussian_2016; @fermanian_single-index_2018]. 

<!--
In mixed company: Bayesian inference for bivariate conditional copula models with discrete and continuous outcomes [@craiu_mixed_2012]

Statistical testing of covariate effects in conditional copula models [@acar_statistical_2013]

Gaussian Process Single Index Models for Conditional Copulas [@levi_gaussian_2016]

Single-index copulas [@fermanian_single-index_2018]
-->

## Extensions
<!-- this section should be about methodological extensions, not applications to clinical trials -->

In addition to the conditional copula models discussed in the previous section, there is extensive literature describing theoretical and methodological extensions related to copula modeling of multivariate survival data [@shih_inferences_1995; @wang_estimating_2003; @he_flexible_2003; @romeo_bivariate_2006; @fu_joint_2013; @lai_mixed_2015; @weber_quantifying_2018], longitudinal or clustered data [@meester_parametric_1994; @eluru_modeling_2010; @madsen_joint_2011; @wu_gaussian_2014; @kwak_estimation_2017; @kwak_estimation_2017-1], joint longitudinal/survival outcomes [@ganjali_copula_2015; @kurum_copula_2018], missing data [@ding_copula_2015; @gomes_copula_2019] and latent factors underlying the dependence structure [@krupskii_factor_2013; @tan_bayesian_2018]. 

Another important area of active research is vine copulas. These models are used to specify multivariate dependence structure with more than 2 dimensions by representing the $d$-dimensional copula in terms of $d \choose 2$ bivariate copulas [@joe_dependence_2015; @smith_modeling_2010; @min_bayesian_2010; @gruber_bayesian_2017; @barthel_dependence_2018]. 

<!-- Survival outcomes

Inferences on the Association Parameter in Copula Models for Bivariate Survival Data [@shih_inferences_1995]

Estimating the association parameter for copula models under dependent censoring [@wang_estimating_2003]

Flexible Maximum Likelihood Methods for Bivariate Proportional Hazards Models [@he_flexible_2003]

Joint modeling of progression-free survival and overall survival by a Bayesian normal induced copula estimation model
[@fu_joint_2013]

Mixed response and time-to-event endpoints for multistage single-arm phase II design
[@lai_mixed_2015]

Quantifying the association between progression-free survival and overall survival in oncology trials using Kendall's $\tau$: Correlation between progression-free survival and overall survival
[@weber_quantifying_2018]

Bivariate Survival Modeling: a Bayesian approach based on Copulas [@romeo_bivariate_2006]

- as alternative to frailty?
(Find book 'The Frailty Model'?) 
-->

<!-- Longitudinal or clustered data can also be analyzed using a copula approach to account for dependence among observations.

Joint Regression Analysis for Discrete Longitudinal Data [@madsen_joint_2011]

Estimation and inference on the joint conditional distribution for bivariate longitudinal data using Gaussian copula [@kwak_estimation_2017] 

Estimation and inference of the joint conditional distribution for multivariate longitudinal data using nonparametric copulas [@kwak_estimation_2017-1]

Gaussian Copula Mixed Models for Clustered Mixed Outcomes, With Application in Developmental Toxicology [@wu_gaussian_2014] - developmental toxicology setting sampling unit is litter of animal fetuses - similar to cluster randomized trial

A Parametric model for cluster correlated categorical data [@meester_parametric_1994]

Modeling Injury Severity of Multiple Occupants of Vehicles: Copula-Based Multivariate Approach [@eluru_modeling_2010]
-->

<!-- Joint longitudinal/survival models

A Copula Approach to Joint Modeling of Longitudinal Measurements and Survival Times Using Monte Carlo Expectation-Maximization with Application to AIDS Studies [@ganjali_copula_2015]

A copula model for joint modeling of longitudinal and time-invariant mixed outcomes: Joint modeling of longitudinal and time-invariant mixed outcomes [@kurum_copula_2018]

* Missing data

Dissertation - Copula Regression Models for the Analysis of Correlated Data with Missing Values [@ding_copula_2015]

Copula selection models for non-Gaussian outcomes that are missing not at random: Copula selection models for non-Normal data [@gomes_copula_2019]

* Factor copula - model dependence between outcomes and latent factors; independent outcome conditional on factor

Factor copula models for multivariate data [@krupskii_factor_2013]

Bayesian Inference for the One-Factor Copula Model [@tan_bayesian_2018]
-->

<!-- Vine copulas - model distributions with >2 dimensions using paired bivariate copulas 

Modeling Longitudinal Data Using a Pair-Copula Decomposition of Serial Dependence [@smith_modeling_2010]

Bayesian inference for multivariate copulas using pair-copula constructions [@min_bayesian_2010]

Bayesian model selection of Regular Vine Copulas [@gruber_bayesian_2017]
-->

Further details on copulas including technical definitions and proofs, methods for copula construction and sampling, extensive descriptions of available copula families and their properties, inference and diagnostic procedures, and computational algorithms are presented in the texts *An Introduction to Copulas* by Roger Nelsen @nelsen_introduction_2006, *Dependence Modeling with Copulas* by Harry Joe @joe_dependence_2015, and *Elements of Copula Modeling with R* by Hofert et al. @hofert_elements_2018. The final text describes the R package `copula` @hofert_copula:_2016 which was used for copula examples throughout this report.
